import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from colorama import Fore, Style, init
import sys
import time

# Renkleri baÅŸlat
init(autoreset=True)

# Ayarlar
BASE_URL = "http://localhost:8000"
CHECK_IMAGES = True
CHECK_CSS = True
CHECK_JS = True
TIMEOUT = 5

visited_pages = set()
broken_links = []
scanned_count = 0

print(Style.BRIGHT + Fore.CYAN + "="*50)
print(Style.BRIGHT + Fore.CYAN + f"ğŸš€ SANTIS V5 ULTRA LINK CHECKER BAÅLATILIYOR...")
print(Style.BRIGHT + Fore.CYAN + f"ğŸ¯ Hedef: {BASE_URL}")
print(Style.BRIGHT + Fore.CYAN + "="*50 + "\n")

def is_internal(url):
    """Linkin site iÃ§i olup olmadÄ±ÄŸÄ±nÄ± kontrol eder."""
    netloc = urlparse(url).netloc
    return netloc == "" or netloc == urlparse(BASE_URL).netloc

def check_url(url, source_page, context="Link"):
    """Bir URL'e istek atÄ±p durumunu kontrol eder."""
    try:
        r = requests.head(url, timeout=TIMEOUT)
        
        # BazÄ± sunucular HEAD kabul etmez, GET deneyelim
        if r.status_code == 405:
            r = requests.get(url, timeout=TIMEOUT, stream=True)
            r.close()

        if r.status_code >= 400:
            print(Fore.RED + f"âŒ [KIRIK] {url}")
            print(Fore.RED + f"   â†³ Kaynak: {source_page} ({context}) -> Kod: {r.status_code}")
            broken_links.append({"url": url, "source": source_page, "code": r.status_code, "context": context})
        else:
            # print(Fore.GREEN + f"âœ… {url}") # Ã‡ok fazla log olmamasÄ± iÃ§in kapalÄ±
            pass

    except Exception as e:
        print(Fore.RED + f"âš ï¸ [HATA] {url}")
        print(Fore.RED + f"   â†³ Kaynak: {source_page} -> {e}")
        broken_links.append({"url": url, "source": source_page, "code": "ERR", "context": context})

def crawl(url):
    """SayfayÄ± tarar ve iÃ§indeki linkleri bulur."""
    global scanned_count
    
    # URL normalizasyonu (Hash ve parametreleri temizle - isteÄŸe baÄŸlÄ±)
    parsed = urlparse(url)
    clean_url = parsed.scheme + "://" + parsed.netloc + parsed.path
    if clean_url in visited_pages:
        return
    
    visited_pages.add(clean_url)
    scanned_count += 1
    
    print(Fore.YELLOW + f"ğŸ” TaranÄ±yor ({scanned_count}): {clean_url}")

    try:
        r = requests.get(url, timeout=TIMEOUT)
        if r.status_code != 200:
            print(Fore.RED + f"âŒ Sayfa AÃ§Ä±lamadÄ±: {url} (Kod: {r.status_code})")
            return

        soup = BeautifulSoup(r.text, "html.parser")

        # 1. <a> Etiketleri (Linkler)
        for a in soup.find_all("a", href=True):
            link = urljoin(url, a["href"])
            
            # Sadece http/https linkleri kontrol et
            if not link.startswith("http"): 
                continue

            if is_internal(link):
                # Ä°Ã§ link ise: Ã–nce kontrol et, sonra kuyruÄŸa ekle (Recursive)
                # Linkin dosya olup olmadÄ±ÄŸÄ±na bak (PDF, JPG vb. crawl edilmez)
                if not any(link.lower().endswith(ext) for ext in ['.jpg', '.png', '.webp', '.pdf', '.css', '.js']):
                     if link not in visited_pages: 
                         # Derinlemesine tarama iÃ§in recursive Ã§aÄŸrÄ± yapmÄ±yoruz, sadece ziyaret listesine ekleyip sonraki adÄ±mda gezebiliriz
                         # Ama bu basit versiyonda sadece check yapÄ±p geÃ§elim, tam site haritasÄ± Ã§Ä±karmak karmaÅŸÄ±k olur.
                         # Åimdilik sadece bulduÄŸu assetleri kontrol etsin.
                         pass
                
                # Her halÃ¼karda linkin Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol et
                check_url(link, url, "Ä°Ã§ Link")
            else:
                # DÄ±ÅŸ link ise sadece kontrol et
                pass # DÄ±ÅŸ linkleri taramak uzun sÃ¼rer, isterseniz aÃ§abilirsiniz.

        # 2. <img> Etiketleri (Resimler)
        if CHECK_IMAGES:
            for img in soup.find_all("img", src=True):
                src = urljoin(url, img["src"])
                check_url(src, url, "GÃ¶rsel")

        # 3. <link> (CSS)
        if CHECK_CSS:
            for css in soup.find_all("link", href=True):
                if "stylesheet" in css.get("rel", []):
                    href = urljoin(url, css["href"])
                    check_url(href, url, "CSS")

        # 4. <script> (JS)
        if CHECK_JS:
            for js in soup.find_all("script", src=True):
                src = urljoin(url, js["src"])
                check_url(src, url, "JS")

    except Exception as e:
        print(Fore.RED + f"âŒ Kritik Hata: {url} -> {e}")

# --- ANA DÃ–NGÃœ ---
if __name__ == "__main__":
    start_time = time.time()
    
    # Ana sayfadan baÅŸla
    crawl(BASE_URL)
    
    # Ekstra olarak bilinen alt sayfalarÄ± da tarama listesine manuel ekleyebiliriz
    # crawl(BASE_URL + "/tr/index.html")
    # crawl(BASE_URL + "/tr/urunler/index.html")

    duration = time.time() - start_time

    print("\n" + Style.BRIGHT + Fore.CYAN + "="*50)
    print(Style.BRIGHT + Fore.CYAN + "ğŸ TARAMA TAMAMLANDI")
    print(Style.BRIGHT + Fore.CYAN + f"â±ï¸ SÃ¼re: {duration:.2f} saniye")
    print(Style.BRIGHT + Fore.CYAN + f"ğŸ“„ Taranan Sayfa: {scanned_count}")
    print(Style.BRIGHT + Fore.CYAN + "="*50)

    if broken_links:
        print(Style.BRIGHT + Fore.RED + f"\nğŸš¨ BULUNAN KIRIK LINKLER ({len(broken_links)} ADET):")
        for error in broken_links:
            print(Fore.RED + f"â€¢ {error['context']}: {error['url']}")
            print(Fore.RED + f"  Kaynak: {error['source']}")
            print(Fore.WHITE + "-"*30)
    else:
        print(Style.BRIGHT + Fore.GREEN + "\nâœ… MÃœKEMMEL! HiÃ§bir kÄ±rÄ±k link bulunamadÄ±.")
