"""
SANTIS ULTRA DEEP SEO AUDIT v1.0
Analyzes: Schema Markup, Breadcrumbs, Orphan Pages, Anchor Text, Sitemap, Listing Pages
"""
import os, re, json, xml.etree.ElementTree as ET
from pathlib import Path
from collections import defaultdict

ROOT = Path(r'c:\Users\tourg\Desktop\SANTIS_SITE')
SKIP_DIRS = {'_legacy_archive','_legacy_content','_snapshots','backup','backups',
             'node_modules','admin','a4','components','venv','__pycache__',
             '.git','.vscode','_dev_archives','print','public','static','templates',
             'includes','reports','sr'}

# Collect all active HTML files
pages = {}
for dp, dn, fn in os.walk(ROOT):
    dn[:] = [d for d in dn if d not in SKIP_DIRS]
    for f in fn:
        if not f.endswith('.html'): continue
        fp = Path(dp) / f
        rel = str(fp.relative_to(ROOT)).replace('\\','/')
        try:
            with open(fp, 'r', encoding='utf-8', errors='ignore') as fh:
                content = fh.read()
            pages[rel] = content
        except:
            pass

print(f'Total active HTML pages: {len(pages)}')
print('='*70)

# â”€â”€â”€ 1. SCHEMA MARKUP ANALYSIS â”€â”€â”€
print('\nğŸ“‹ 1. SCHEMA MARKUP ANALÄ°ZÄ°')
print('-'*70)
schema_pages = {}
no_schema = []
for rel, content in pages.items():
    schemas = re.findall(r'<script\s+type=["\']application/ld\+json["\']>(.*?)</script>', content, re.DOTALL|re.IGNORECASE)
    if schemas:
        types = []
        for s in schemas:
            try:
                data = json.loads(s)
                t = data.get('@type','?')
                types.append(t)
            except:
                types.append('PARSE_ERROR')
        schema_pages[rel] = types
    else:
        no_schema.append(rel)

print(f'  Schema olan: {len(schema_pages)} sayfa')
print(f'  Schema OLMAYAN: {len(no_schema)} sayfa')

# Schema type distribution
type_counts = defaultdict(int)
for types in schema_pages.values():
    for t in types:
        type_counts[t] += 1
print(f'\n  Schema Tip DaÄŸÄ±lÄ±mÄ±:')
for t, c in sorted(type_counts.items(), key=lambda x: -x[1]):
    print(f'    {t}: {c} sayfa')

print(f'\n  Schema OLMAYAN Ã¶nemli sayfalar:')
for p in no_schema:
    if not p.startswith(('de/','en/','fr/','ru/')):
        print(f'    âŒ {p}')

# â”€â”€â”€ 2. BREADCRUMB ANALYSIS â”€â”€â”€
print('\n\nğŸ 2. BREADCRUMB ANALÄ°ZÄ°')
print('-'*70)
has_breadcrumb_html = 0
has_breadcrumb_schema = 0
no_breadcrumb = []
for rel, content in pages.items():
    has_bc_html = bool(re.search(r'breadcrumb|aria-label=["\']breadcrumb', content, re.IGNORECASE))
    has_bc_schema = 'BreadcrumbList' in content
    if has_bc_html: has_breadcrumb_html += 1
    if has_bc_schema: has_breadcrumb_schema += 1
    if not has_bc_html and not has_bc_schema:
        no_breadcrumb.append(rel)

print(f'  HTML breadcrumb olan: {has_breadcrumb_html}')
print(f'  Schema breadcrumb olan: {has_breadcrumb_schema}')
print(f'  Breadcrumb OLMAYAN: {len(no_breadcrumb)} sayfa')
print(f'  â†’ TÃ¼m sayfalarda breadcrumb EKSIK' if len(no_breadcrumb) == len(pages) else '')

# â”€â”€â”€ 3. ORPHAN PAGE ANALYSIS â”€â”€â”€
print('\n\nğŸ”— 3. ORPHAN PAGE (YETÄ°M SAYFA) ANALÄ°ZÄ°')
print('-'*70)
# Build link graph
all_hrefs = defaultdict(set)  # page -> set of pages it links to
linked_by = defaultdict(set)  # page -> set of pages linking to it

for rel, content in pages.items():
    hrefs = re.findall(r'href=["\']([^"\'#]+)', content, re.IGNORECASE)
    for href in hrefs:
        # Normalize href
        href = href.strip()
        if href.startswith(('http','mailto','tel','javascript','data:')): continue
        if href.startswith('/'):
            target = href.lstrip('/')
        else:
            # Relative path
            base_dir = os.path.dirname(rel)
            target = os.path.normpath(os.path.join(base_dir, href)).replace('\\','/')
        # Remove query strings
        target = target.split('?')[0]
        if target in pages:
            all_hrefs[rel].add(target)
            linked_by[target].add(rel)

orphans = []
for rel in pages:
    if rel not in linked_by or len(linked_by[rel]) == 0:
        orphans.append(rel)

# Also check pages with very few incoming links
low_incoming = []
for rel in pages:
    incoming = len(linked_by.get(rel, set()))
    if 0 < incoming <= 1:
        low_incoming.append((rel, incoming))

print(f'  Yetim sayfa (0 gelen link): {len(orphans)}')
for p in sorted(orphans):
    if not p.startswith(('de/','fr/','ru/')):
        print(f'    ğŸš« {p}')

print(f'\n  ZayÄ±f baÄŸlÄ± sayfa (1 gelen link): {len(low_incoming)}')
for p, c in sorted(low_incoming)[:15]:
    if not p.startswith(('de/','fr/','ru/')):
        print(f'    âš ï¸ {p} ({c} link)')

# â”€â”€â”€ 4. ANCHOR TEXT ANALYSIS â”€â”€â”€
print('\n\nğŸ·ï¸ 4. ANCHOR TEXT ANALÄ°ZÄ°')
print('-'*70)
anchor_texts = defaultdict(list)  # anchor_text -> [(source, target)]
for rel, content in pages.items():
    links = re.findall(r'<a\s[^>]*href=["\']([^"\']+)["\'][^>]*>(.*?)</a>', content, re.IGNORECASE|re.DOTALL)
    for href, text in links:
        if href.startswith(('http','mailto','tel','javascript','#')): continue
        clean_text = re.sub(r'<[^>]+>', '', text).strip()
        if clean_text and len(clean_text) > 1:
            anchor_texts[clean_text.lower()].append((rel, href))

# Find over-used anchors
print(f'  Toplam benzersiz anchor text: {len(anchor_texts)}')
print(f'\n  En Ã§ok kullanÄ±lan anchor textler:')
sorted_anchors = sorted(anchor_texts.items(), key=lambda x: -len(x[1]))
for text, usages in sorted_anchors[:15]:
    print(f'    "{text}" â†’ {len(usages)} kez kullanÄ±lmÄ±ÅŸ')

# Find generic/weak anchors
generic = ['tÃ¼mÃ¼nÃ¼ gÃ¶r','devamÄ±nÄ± oku','tÄ±klayÄ±n','click here','read more','learn more','daha fazla','buraya tÄ±klayÄ±n','detay','incele']
print(f'\n  ZayÄ±f/Generic anchor text:')
for g in generic:
    if g in anchor_texts:
        print(f'    âš ï¸ "{g}" â†’ {len(anchor_texts[g])} kez')

# â”€â”€â”€ 5. SITEMAP ANALYSIS â”€â”€â”€
print('\n\nğŸ—ºï¸ 5. SITEMAP ANALÄ°ZÄ°')
print('-'*70)
sitemap_path = ROOT / 'sitemap.xml'
if sitemap_path.exists():
    with open(sitemap_path, 'r', encoding='utf-8') as f:
        sitemap_content = f.read()
    print(f'  Dosya boyutu: {len(sitemap_content)} byte')
    
    # Count URLs
    urls_in_sitemap = re.findall(r'<loc>(.*?)</loc>', sitemap_content)
    print(f'  URL sayÄ±sÄ±: {len(urls_in_sitemap)}')
    
    # Check lastmod
    lastmods = re.findall(r'<lastmod>(.*?)</lastmod>', sitemap_content)
    print(f'  lastmod olan: {len(lastmods)}')
    
    # Check priority
    priorities = re.findall(r'<priority>(.*?)</priority>', sitemap_content)
    print(f'  priority olan: {len(priorities)}')
    
    # Check changefreq
    changefreqs = re.findall(r'<changefreq>(.*?)</changefreq>', sitemap_content)
    print(f'  changefreq olan: {len(changefreqs)}')
    
    # Pages NOT in sitemap
    sitemap_paths = set()
    for url in urls_in_sitemap:
        # Extract path from URL
        path = url.replace('https://santis.club/','').replace('http://santis.club/','')
        sitemap_paths.add(path)
    
    not_in_sitemap = [p for p in pages if p not in sitemap_paths]
    print(f'\n  Sitemap\'te OLMAYAN sayfa: {len(not_in_sitemap)}')
    for p in sorted(not_in_sitemap)[:10]:
        print(f'    âŒ {p}')
    if len(not_in_sitemap) > 10:
        print(f'    ... ve {len(not_in_sitemap)-10} sayfa daha')
    
    print(f'\n  Sitemap Ä°Ã§eriÄŸi:')
    print(f'  {sitemap_content[:500]}')
else:
    print('  âŒ sitemap.xml BULUNAMADI!')

# â”€â”€â”€ 6. LISTING PAGE (24 SHORT PAGE) ANALYSIS â”€â”€â”€
print('\n\nğŸ“„ 6. LISTING SAYFA ANALÄ°ZÄ° (KÄ±sa Ä°Ã§erikli Hub SayfalarÄ±)')
print('-'*70)
listing_pages = []
for rel, content in pages.items():
    if not (rel.startswith('tr/') or rel in ('index.html','booking.html','service-detail.html','showroom.html','404.html','kese-ve-kopuk-masaji.html')):
        continue
    clean = re.sub(r'<script[^>]*>.*?</script>','',content,flags=re.I|re.DOTALL)
    clean = re.sub(r'<style[^>]*>.*?</style>','',clean,flags=re.I|re.DOTALL)
    tx = re.sub(r'<[^>]+>',' ',clean)
    tx = re.sub(r'\s+',' ',tx).strip()
    wc = len(tx.split())
    if wc < 80:
        title = re.search(r'<title>(.*?)</title>', content, re.I)
        title_text = title.group(1).strip() if title else '?'
        has_faq = bool(re.search(r'FAQ|faq|sss|sÄ±k sorulan', content, re.I))
        has_intro = bool(re.search(r'class=".*intro|description|summary', content, re.I))
        listing_pages.append({
            'file': rel,
            'words': wc,
            'title': title_text,
            'has_faq': has_faq,
            'has_intro': has_intro,
        })

print(f'  KÄ±sa listing sayfa sayÄ±sÄ±: {len(listing_pages)}')
for p in listing_pages:
    faq_icon = 'âœ…' if p['has_faq'] else 'âŒ'
    intro_icon = 'âœ…' if p['has_intro'] else 'âŒ'
    print(f'    {p["file"]} ({p["words"]}w) | FAQ:{faq_icon} Intro:{intro_icon}')

# â”€â”€â”€ 7. ROBOTS.TXT ANALYSIS â”€â”€â”€
print('\n\nğŸ¤– 7. ROBOTS.TXT ANALÄ°ZÄ°')
print('-'*70)
robots_path = ROOT / 'robots.txt'
if robots_path.exists():
    with open(robots_path, 'r') as f:
        robots_content = f.read()
    print(f'  Ä°Ã§erik:\n{robots_content}')
else:
    print('  âŒ robots.txt BULUNAMADI!')

# â”€â”€â”€ 8. CANONICAL & HREFLANG ANALYSIS â”€â”€â”€
print('\n\nğŸŒ 8. CANONICAL & HREFLANG ANALÄ°ZÄ°')
print('-'*70)
no_canonical = []
no_hreflang = []
for rel, content in pages.items():
    has_canonical = bool(re.search(r'rel=["\']canonical["\']', content, re.I))
    has_hreflang = bool(re.search(r'hreflang=', content, re.I))
    if not has_canonical: no_canonical.append(rel)
    if not has_hreflang: no_hreflang.append(rel)

print(f'  Canonical OLMAYAN: {len(no_canonical)} sayfa')
for p in no_canonical[:10]:
    print(f'    âŒ {p}')
print(f'  Hreflang OLMAYAN: {len(no_hreflang)} sayfa')
for p in no_hreflang[:10]:
    print(f'    âŒ {p}')

# â”€â”€â”€ 9. META DESCRIPTION ANALYSIS â”€â”€â”€
print('\n\nğŸ“ 9. META DESCRIPTION ANALÄ°ZÄ°')
print('-'*70)
no_desc = []
short_desc = []
for rel, content in pages.items():
    desc_match = re.search(r'<meta\s[^>]*name=["\']description["\'][^>]*content=["\']([^"\']*)["\']', content, re.I)
    if not desc_match:
        desc_match = re.search(r'<meta\s[^>]*content=["\']([^"\']*)["\'][^>]*name=["\']description["\']', content, re.I)
    if not desc_match:
        no_desc.append(rel)
    elif len(desc_match.group(1).strip()) < 50:
        short_desc.append((rel, desc_match.group(1).strip()))

print(f'  Meta description OLMAYAN: {len(no_desc)} sayfa')
for p in no_desc[:10]:
    print(f'    âŒ {p}')
print(f'  KÄ±sa meta description (<50 char): {len(short_desc)} sayfa')
for p, d in short_desc[:5]:
    print(f'    âš ï¸ {p}: "{d}"')

# â”€â”€â”€ 10. OPEN GRAPH ANALYSIS â”€â”€â”€
print('\n\nğŸ“± 10. OPEN GRAPH ANALÄ°ZÄ°')
print('-'*70)
no_og_title = 0
no_og_desc = 0
no_og_image = 0
for rel, content in pages.items():
    if not re.search(r'property=["\']og:title', content, re.I): no_og_title += 1
    if not re.search(r'property=["\']og:description', content, re.I): no_og_desc += 1
    if not re.search(r'property=["\']og:image', content, re.I): no_og_image += 1

print(f'  og:title eksik: {no_og_title} sayfa')
print(f'  og:description eksik: {no_og_desc} sayfa')
print(f'  og:image eksik: {no_og_image} sayfa')

print('\n\n' + '='*70)
print('TARAMA TAMAMLANDI')
print('='*70)
