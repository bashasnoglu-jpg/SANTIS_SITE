import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import logging
from collections import deque
import shutil
import re
import glob
import os
from pathlib import Path

# Logging setup
logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger("DeepAudit")

try:
    from semantic_audit import SemanticAuditEngine
except ImportError:
    SemanticAuditEngine = None

class DeepAuditEngine:
    def __init__(self, base_url, max_depth=5, max_pages=1000, rate_limit=0.1):
        self.base_url = base_url.rstrip("/")
        self.max_depth = max_depth
        self.max_pages = max_pages
        self.rate_limit = rate_limit
        
        self.status = "IDLE" # IDLE, RUNNING, COMPLETED, STOPPED
        self.visited = set()
        self.queue = deque([(self.base_url, 0)]) # (url, depth)
        
        # Report Data
        self.broken_links = []
        self.missing_assets = [] # css, js, img
        self.server_errors = []
        self.seo_issues = []
        self.fix_suggestions = []
        
        # Ultra Mega: Semantic Audit
        self.semantic_results = []
        self.semantic_engine = SemanticAuditEngine() if SemanticAuditEngine else None
        
        # Progress Tracking
        self.total_discovered = 1 # Start with base_url
        self.scanned_count = 0

        
        # Headers to mimic a real browser to avoid 403s
        self.headers = {
            "User-Agent": "SantisSentinel/2.0 (Deep Audit Bot)"
        }

    def _is_internal(self, url):
        return url.startswith(self.base_url)

    def _check_url(self, url, retries=2):
        """Checks a URL status with retry logic."""
        for attempt in range(retries + 1):
            try:
                r = requests.head(url, headers=self.headers, timeout=5, allow_redirects=True)
                if r.status_code == 405: # Method Not Allowed -> Try GET
                    r = requests.get(url, headers=self.headers, timeout=5, stream=True)
                return r.status_code, r
            except requests.RequestException:
                if attempt == retries:
                    return 0, None # 0 indicates connection error
                time.sleep(0.5) # Wait before retry

    def _validate_asset(self, url, source_page):
        status, _ = self._check_url(url)
        if status != 200:
            self.missing_assets.append({
                "url": url,
                "source": source_page,
                "status": status if status != 0 else "CONN_ERR"
            })

    def _analyze_seo(self, soup, url):
        title = soup.find("title")
        if not title or not title.string or not title.string.strip():
            self.seo_issues.append({"url": url, "issue": "Missing Title"})
        
        desc = soup.find("meta", attrs={"name": "description"})
        if not desc or not desc.get("content"):
            self.seo_issues.append({"url": url, "issue": "Missing Description"})

    def _suggest_fix(self, broken_url):
        """Simple heuristic to suggest fixes for broken links."""
        try:
            parsed = urlparse(broken_url)
            path = parsed.path
            
            # 1. Typo in extension (.html -> .htm)
            if path.endswith(".htm"):
                return broken_url.replace(".htm", ".html")
            
            # 2. Missing index.html for directories
            if not path.endswith(".html") and not path.endswith("/"):
                 return broken_url + "/index.html"
                 
            # 3. Path reconstruction (parent dir)
            parts = path.rstrip("/").split("/")
            if len(parts) > 1:
                return f"{parsed.scheme}://{parsed.netloc}{'/'.join(parts[:-1])}/index.html"
                
        except:
            pass
        return None

    def run(self):
        logger.info(f"ðŸš€ Starting Deep Audit on {self.base_url} (Depth: {self.max_depth})")
        self.status = "RUNNING"
        self.scanned_count = 0
        
        while self.queue and self.scanned_count < self.max_pages:
            # Rate limit
            if self.rate_limit > 0:
                time.sleep(self.rate_limit)

            url, depth = self.queue.popleft()
            
            if url in self.visited:
                continue
            self.visited.add(url)
            self.scanned_count += 1
            
            # Fetch Page
            status, response = self._check_url(url)
            
            if status >= 500:
                self.server_errors.append({"url": url, "status": status})
                continue
                
            if status != 200:
                self.broken_links.append({"url": url, "status": status})
                # Attempt fix suggestion
                fix = self._suggest_fix(url)
                if fix:
                    self.fix_suggestions.append({"broken": url, "suggestion": fix})
                continue

            # Only parse HTML for further crawling
            content_type = response.headers.get("Content-Type", "")
            if "text/html" not in content_type:
                continue
                
            try:
                # Re-fetch with GET for body content if we only did HEAD/Stream
                if not response.content: 
                     _, response = self._check_url(url) # Ensure we have content
                     
                soup = BeautifulSoup(response.text, "html.parser")
                
                # 1. SEO Analysis
                if self._is_internal(url):
                    self._analyze_seo(soup, url)

                # ðŸš€ ULTRA MEGA: Semantic Audit (Tone Guard)
                if self.semantic_engine and self._is_internal(url):
                    text_content = soup.get_text(" ", strip=True)
                    analysis = self.semantic_engine.analyze_text(text_content)
                    if analysis:
                        analysis["url"] = url
                        self.semantic_results.append(analysis)
                 
                # 2. Extract & Validate Assets (Images, Scripts, CSS)
                # Images
                for img in soup.find_all("img", src=True):
                    asset_url = urljoin(url, img["src"])
                    self._validate_asset(asset_url, url)
                    
                # Scripts
                for script in soup.find_all("script", src=True):
                    asset_url = urljoin(url, script["src"])
                    if self._is_internal(asset_url): 
                        self._validate_asset(asset_url, url)
                        
                # CSS
                for link in soup.find_all("link", rel="stylesheet", href=True):
                    asset_url = urljoin(url, link["href"])
                    if self._is_internal(asset_url):
                        self._validate_asset(asset_url, url)

                # 3. Extract Links for Recursion
                if depth < self.max_depth:
                    for a in soup.find_all("a", href=True):
                        link = urljoin(url, a["href"])
                        link = link.split("#")[0]
                        
                        if self._is_internal(link) and link not in self.visited:
                            # Only add if not already in queue (check handled effectively by set in robust implementations, 
                            # here we just append, effectively BFS)
                            self.queue.append((link, depth + 1))
                            self.total_discovered += 1
                            
            except Exception as e:
                logger.error(f"Error parsing {url}: {e}")
                self.server_errors.append({"url": url, "status": f"PARSE_ERR: {str(e)}"})

        self.status = "COMPLETED"
        logger.info(f"âœ… Audit Complete. Scanned {self.scanned_count} pages.")
        
        return self.get_report()

    def get_report(self):
        """Returns the current audit report."""
        return {
            "status": self.status,
            "summary": {
                "scanned_pages": self.scanned_count,
                "broken_links_count": len(self.broken_links),
                "missing_assets_count": len(self.missing_assets),
                "server_errors_count": len(self.server_errors),
                "seo_issues_count": len(self.seo_issues)
            },
            "broken_links": self.broken_links,
            "missing_assets": self.missing_assets,
            "server_errors": self.server_errors,
            "seo_issues": self.seo_issues,
            "fix_suggestions": self.fix_suggestions,
            "semantic_audit": self.semantic_results
        }

    # --- AUTO-FIX & OPTIMIZATION MODULES ---

    def _backup_file(self, file_path):
        """Creates a .bak copy of the file before modification."""
        try:
            path = Path(file_path)
            if path.exists():
                backup_path = path.with_suffix(path.suffix + ".bak")
                shutil.copy2(path, backup_path)
                logger.info(f"Backup created: {backup_path}")
                return True
        except Exception as e:
            logger.error(f"Backup failed for {file_path}: {e}")
            return False

    def heal_missing_images(self, placeholder_path="assets/img/placeholder_santis.jpg"):
        """
        Copies a placeholder image to the location of missing images.
        """
        healed_count = 0
        if not os.path.exists(placeholder_path):
            logger.error(f"Placeholder image not found at {placeholder_path}")
            return {"status": "error", "msg": "Placeholder source missing"}

        for item in self.missing_assets:
            url = item.get("url", item)
            # Only fix local images
            if not self._is_internal(url):
                continue
            
            # Convert URL to local path
            parsed = urlparse(url)
            path_str = parsed.path.lstrip("/")
            local_path = Path(path_str)
            
            # Additional safety: Don't write outside root
            if ".." in path_str or ":" in path_str:
                continue

            try:
                # Ensure directory exists
                local_path.parent.mkdir(parents=True, exist_ok=True)
                
                # Copy placeholder
                shutil.copy2(placeholder_path, local_path)
                healed_count += 1
                logger.info(f"Healed missing image: {local_path}")
                
                # Update status in report
                item["status"] = "HEALED"
                
            except Exception as e:
                logger.error(f"Failed to heal {url}: {e}")
        
        return {"status": "success", "healed_count": healed_count}

    def fix_broken_links(self):
        """
        Attempts to fix broken links in HTML files by appending .html extension if applicable.
        """
        fixed_count = 0
        # Group broken links by source page (Referer needed)
        # Note: In current logic we store broken links but not explicitly the source page in a structured way for *every* occurrence if we hit it multiple times, 
        # but let's assume we scan files. 
        # BETTER STRATEGY: Scan all discovered HTML files and replace known broken links.
        
        known_broken = {item.get("url") for item in self.broken_links}
        
        # 1. Identify files to patch (from visited set)
        local_html_files = []
        for url in self.visited:
            if self._is_internal(url):
                parsed = urlparse(url)
                path = parsed.path.lstrip("/")
                if not path: path = "index.html"
                if path.endswith("/"): path += "index.html"
                
                if path.endswith(".html") and os.path.exists(path):
                    local_html_files.append(path)

        # 2. Process each file
        for file_path in local_html_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                
                original_content = content
                modified = False
                
                # Simple regex replace for known broken links to .html versions
                # Logic: If 'contact' is broken and 'contact.html' exists, replace href="contact" with href="contact.html"
                
                # Find all hrefs
                soup = BeautifulSoup(content, "html.parser")
                for a in soup.find_all("a", href=True):
                    href = a["href"]
                    full_url = urljoin(self.base_url, href)
                    
                    if full_url in known_broken:
                        # Check if adding .html fixes it
                        # This is a heuristic. 
                        # A better way: Check if we have a fix suggestion for this URL
                        
                        suggestion = None
                        for sug in self.fix_suggestions:
                             if sug["broken"] == full_url:
                                 suggestion = sug["suggestion"]
                                 break
                        
                        if suggestion:
                            # Calculate relative path from current file to suggestion
                            # This is complex to get right 100% with urljoin logic reversed.
                            # Simplified: If suggestion ends in .html and original didn't, and they share base name.
                            
                            if suggestion.endswith(".html") and not href.endswith(".html"):
                                # Check if it's the same base
                                if suggestion.replace(".html", "") == full_url: 
                                    # It's a match!
                                    new_href = href + ".html"
                                    # Specific replacement in content (careful with multiple occurrences)
                                    # Use regex to ensure we match specific href attributes
                                    # pattern = f'href=["\']{re.escape(href)}["\']'
                                    # content = re.sub(pattern, f'href="{new_href}"', content)
                                    # Let's use string replace for simplicity but caution
                                    
                                    # Verify usage context to avoid replacing substrings
                                    quote_types = ['"', "'"]
                                    for q in quote_types:
                                        target = f'href={q}{href}{q}'
                                        replacement = f'href={q}{new_href}{q}'
                                        if target in content:
                                            content = content.replace(target, replacement)
                                            modified = True
                                            fixed_count += 1

                if modified:
                    self._backup_file(file_path)
                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(content)
                    logger.info(f"Fixed links in {file_path}")

            except Exception as e:
                logger.error(f"Error processing {file_path}: {e}")

        return {"status": "success", "fixed_count": fixed_count}

    def generate_sitemap(self):
        """
        Generates sitemap.xml from visited internal links.
        """
        sitemap_path = "sitemap.xml"
        try:
            with open(sitemap_path, "w", encoding="utf-8") as f:
                f.write('<?xml version="1.0" encoding="UTF-8"?>\n')
                f.write('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n')
                
                for url in self.visited:
                    if self._is_internal(url):
                        f.write('  <url>\n')
                        f.write(f'    <loc>{url}</loc>\n')
                        # f.write(f'    <lastmod>{time.strftime("%Y-%m-%d")}</lastmod>\n')
                        f.write('    <changefreq>daily</changefreq>\n')
                        f.write('  </url>\n')
                
                f.write('</urlset>')
            
            logger.info(f"Sitemap generated at {sitemap_path}")
            return {"status": "success", "path": sitemap_path, "url_count": len(self.visited)}
            
        except Exception as e:
            logger.error(f"Sitemap generation failed: {e}")
            return {"status": "error", "msg": str(e)}

    def find_unused_assets(self, asset_dir="assets/img"):
        """
        Compares disk files vs crawled assets to find ghosts.
        """
        unused = []
        try:
            # 1. Gather all files on disk
            # Recursively find all files in asset_dir
            disk_files = set()
            for root, dirs, files in os.walk(asset_dir):
                for file in files:
                    full_path = os.path.join(root, file)
                    # Normalize path separators
                    normalized = str(Path(full_path)).replace("\\", "/")
                    disk_files.add(normalized)
            
            # 2. Gather used assets (from crawl)
            # We need to collect valid assets too, not just missing/broken
            # Since current run() doesn't store 'valid' assets explicitly in a list (only missing),
            # we need to ensure we tracked them. 
            # Ideally, we should add a `self.discovered_assets` set in `run()`
            
            # For now, let's assume we can't fully do this without modifying run() to track ALL assets.
            # But let's add a placeholder return or simple logic if `self.visited` implicitly covers pages.
            # This is risky without strict asset tracking.
            
            return {"status": "skipped", "msg": "Requires Asset Tracking enabled"}

        except Exception as e:
            return {"status": "error", "msg": str(e)}

# Self-test
if __name__ == "__main__":
    import sys
    url = sys.argv[1] if len(sys.argv) > 1 else "http://localhost:8000"
    engine = DeepAuditEngine(url, max_depth=2)
    report = engine.run()
    import json
    print(json.dumps(report, indent=2))
